{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/axel-sirota/nlp-and-transformers/blob/main/module3/NLPTransformers_Mod3Demo3_AttentionKeras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r45tbR6lkYEa"
   },
   "source": [
    "# Attention with Keras\n",
    "\n",
    "© Data Trainers LLC. GPL v 3.0.\n",
    "\n",
    "Author: Axel Sirota"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hcU-PBR3kbaL"
   },
   "source": [
    "A whole new world opportunities appear when considering using the layer implementations of the attention components. As of July 2023 we have 3 layers implemented:\n",
    "\n",
    "- AdditiveAttention: This is the original Attention from the Bahdanau paper that incorporates the concept of Q,K,V attention we say in demo 2; setting, in this case, K=V.\n",
    "- Attention: This is the Dot Product attention from *Luong et. al.* we saw in the first demo.\n",
    "- MultiHeadAttention: The general attention everyone uses and we will learn in this demo! It is basically many layers of self attention.\n",
    "\n",
    "Let's get to it!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JsTW2QHimUlx"
   },
   "source": [
    "## Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "C9BTEOu0PerV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/app-root/lib/python3.9/site-packages (3.8.1)\n",
      "Collecting gensim==4.2.0\n",
      "  Downloading gensim-4.2.0-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (24.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.0/24.0 MB\u001b[0m \u001b[31m111.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting keras-nlp\n",
      "  Downloading keras_nlp-0.8.2-py3-none-any.whl (465 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m465.3/465.3 kB\u001b[0m \u001b[31m176.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting keras-preprocessing\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m127.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow-text>=2.11\n",
      "  Downloading tensorflow_text-2.15.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m130.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17.0 in /opt/app-root/lib/python3.9/site-packages (from gensim==4.2.0) (1.24.4)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/app-root/lib/python3.9/site-packages (from gensim==4.2.0) (6.4.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /opt/app-root/lib/python3.9/site-packages (from gensim==4.2.0) (1.11.4)\n",
      "Requirement already satisfied: tqdm in /opt/app-root/lib/python3.9/site-packages (from nltk) (4.66.1)\n",
      "Requirement already satisfied: click in /opt/app-root/lib/python3.9/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/app-root/lib/python3.9/site-packages (from nltk) (2023.12.25)\n",
      "Requirement already satisfied: joblib in /opt/app-root/lib/python3.9/site-packages (from nltk) (1.3.2)\n",
      "Collecting keras-core\n",
      "  Downloading keras_core-0.1.7-py3-none-any.whl (950 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m950.8/950.8 kB\u001b[0m \u001b[31m179.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: rich in /opt/app-root/lib/python3.9/site-packages (from keras-nlp) (12.6.0)\n",
      "Collecting kagglehub\n",
      "  Downloading kagglehub-0.2.0-py3-none-any.whl (33 kB)\n",
      "Requirement already satisfied: packaging in /opt/app-root/lib/python3.9/site-packages (from keras-nlp) (23.2)\n",
      "Collecting dm-tree\n",
      "  Downloading dm_tree-0.1.8-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (153 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.0/153.0 kB\u001b[0m \u001b[31m160.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: absl-py in /opt/app-root/lib/python3.9/site-packages (from keras-nlp) (1.4.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/app-root/lib/python3.9/site-packages (from keras-preprocessing) (1.16.0)\n",
      "Collecting tensorflow<2.16,>=2.15.0\n",
      "  Downloading tensorflow-2.15.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m154.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow-hub>=0.13.0\n",
      "  Downloading tensorflow_hub-0.16.1-py2.py3-none-any.whl (30 kB)\n",
      "Collecting flatbuffers>=23.5.26\n",
      "  Downloading flatbuffers-24.3.7-py2.py3-none-any.whl (26 kB)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3\n",
      "  Downloading protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m167.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /opt/app-root/lib/python3.9/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text>=2.11) (2.4.0)\n",
      "Requirement already satisfied: setuptools in /opt/app-root/lib/python3.9/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text>=2.11) (68.1.2)\n",
      "Collecting wrapt<1.15,>=1.11.0\n",
      "  Downloading wrapt-1.14.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m147.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard<2.16,>=2.15\n",
      "  Downloading tensorboard-2.15.2-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m150.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting libclang>=13.0.0\n",
      "  Downloading libclang-16.0.6-py2.py3-none-manylinux2010_x86_64.whl (22.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.9/22.9 MB\u001b[0m \u001b[31m151.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting keras<2.16,>=2.15.0\n",
      "  Downloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m171.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting h5py>=2.9.0\n",
      "  Downloading h5py-3.10.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m113.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting ml-dtypes~=0.3.1\n",
      "  Downloading ml_dtypes-0.3.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m165.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.36.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m158.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/app-root/lib/python3.9/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text>=2.11) (4.9.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/app-root/lib/python3.9/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text>=2.11) (1.60.0)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m153.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1\n",
      "  Downloading gast-0.5.4-py3-none-any.whl (19 kB)\n",
      "Collecting tensorflow-estimator<2.16,>=2.15.0\n",
      "  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.0/442.0 kB\u001b[0m \u001b[31m183.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m135.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tf-keras>=2.14.1\n",
      "  Downloading tf_keras-2.16.0-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m168.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /opt/app-root/lib/python3.9/site-packages (from kagglehub->keras-nlp) (2.31.0)\n",
      "Collecting namex\n",
      "  Downloading namex-0.0.7-py3-none-any.whl (5.8 kB)\n",
      "Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in /opt/app-root/lib/python3.9/site-packages (from rich->keras-nlp) (0.9.1)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /opt/app-root/lib/python3.9/site-packages (from rich->keras-nlp) (2.17.2)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/app-root/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow<2.16,>=2.15.0->tensorflow-text>=2.11) (0.41.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/app-root/lib/python3.9/site-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text>=2.11) (0.7.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /opt/app-root/lib/python3.9/site-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text>=2.11) (1.0.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/app-root/lib/python3.9/site-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text>=2.11) (3.0.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/app-root/lib/python3.9/site-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text>=2.11) (2.27.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/app-root/lib/python3.9/site-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text>=2.11) (3.5.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/app-root/lib/python3.9/site-packages (from requests->kagglehub->keras-nlp) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/app-root/lib/python3.9/site-packages (from requests->kagglehub->keras-nlp) (3.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/app-root/lib/python3.9/site-packages (from requests->kagglehub->keras-nlp) (2023.11.17)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/app-root/lib/python3.9/site-packages (from requests->kagglehub->keras-nlp) (1.26.18)\n",
      "Collecting tf-keras>=2.14.1\n",
      "  Downloading tf_keras-2.15.1-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m189.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/app-root/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text>=2.11) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/app-root/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text>=2.11) (4.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/app-root/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text>=2.11) (5.3.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/app-root/lib/python3.9/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text>=2.11) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/app-root/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text>=2.11) (7.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/app-root/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text>=2.11) (2.1.4)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/app-root/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text>=2.11) (3.17.0)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/app-root/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text>=2.11) (0.5.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/app-root/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text>=2.11) (3.2.2)\n",
      "Installing collected packages: namex, libclang, flatbuffers, dm-tree, wrapt, tensorflow-io-gcs-filesystem, tensorflow-estimator, protobuf, opt-einsum, ml-dtypes, keras-preprocessing, keras, h5py, google-pasta, gast, astunparse, keras-core, kagglehub, gensim, tensorboard, tensorflow, tf-keras, tensorflow-hub, tensorflow-text, keras-nlp\n",
      "  Attempting uninstall: wrapt\n",
      "    Found existing installation: wrapt 1.16.0\n",
      "    Uninstalling wrapt-1.16.0:\n",
      "      Successfully uninstalled wrapt-1.16.0\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.2\n",
      "    Uninstalling protobuf-3.20.2:\n",
      "      Successfully uninstalled protobuf-3.20.2\n",
      "  Attempting uninstall: gensim\n",
      "    Found existing installation: gensim 4.3.2\n",
      "    Uninstalling gensim-4.3.2:\n",
      "      Successfully uninstalled gensim-4.3.2\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.13.0\n",
      "    Uninstalling tensorboard-2.13.0:\n",
      "      Successfully uninstalled tensorboard-2.13.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "onnxconverter-common 1.14.0 requires protobuf==3.20.2, but you have protobuf 4.25.3 which is incompatible.\n",
      "mysql-connector-python 8.0.33 requires protobuf<=3.20.3,>=3.11.0, but you have protobuf 4.25.3 which is incompatible.\n",
      "kfp 1.8.22 requires protobuf<4,>=3.13.0, but you have protobuf 4.25.3 which is incompatible.\n",
      "kfp-pipeline-spec 0.1.16 requires protobuf<4,>=3.13.0, but you have protobuf 4.25.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed astunparse-1.6.3 dm-tree-0.1.8 flatbuffers-24.3.7 gast-0.5.4 gensim-4.2.0 google-pasta-0.2.0 h5py-3.10.0 kagglehub-0.2.0 keras-2.15.0 keras-core-0.1.7 keras-nlp-0.8.2 keras-preprocessing-1.1.2 libclang-16.0.6 ml-dtypes-0.3.2 namex-0.0.7 opt-einsum-3.3.0 protobuf-4.25.3 tensorboard-2.15.2 tensorflow-2.15.1 tensorflow-estimator-2.15.0 tensorflow-hub-0.16.1 tensorflow-io-gcs-filesystem-0.36.0 tensorflow-text-2.15.0 tf-keras-2.15.1 wrapt-1.14.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -U nltk 'gensim==4.2.0' 'keras-nlp' 'keras-preprocessing' 'tensorflow-text>=2.11'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9H2IYs8QkXZd"
   },
   "source": [
    "Let's run some helper functions to setup using the GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "0fpgYwAtNO2T"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-11 05:03:21.015893: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-11 05:03:21.067546: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-11 05:03:21.067591: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-11 05:03:21.069099: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-11 05:03:21.076985: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-11 05:03:21.077691: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-11 05:03:22.249510: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'textblob'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m preprocessing\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtextblob\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TextBlob, Word\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras_preprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequence\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pad_sequences\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minitializers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Constant\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'textblob'"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Lambda, ELU, Conv1D, MaxPooling1D, Dropout\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import preprocessing\n",
    "from textblob import TextBlob, Word\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.initializers import Constant\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from tensorflow.keras import Model, Input\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import warnings\n",
    "import nltk\n",
    "import time\n",
    "\n",
    "TRACE = False\n",
    "\n",
    "def set_seeds_and_trace():\n",
    "  os.environ['PYTHONHASHSEED'] = '0'\n",
    "  np.random.seed(42)\n",
    "  tf.random.set_seed(42)\n",
    "  random.seed(42)\n",
    "  if TRACE:\n",
    "    tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "def set_session_with_gpus_and_cores():\n",
    "  cores = multiprocessing.cpu_count()\n",
    "  gpus = len(tf.config.list_physical_devices('GPU'))\n",
    "  config = tf.compat.v1.ConfigProto( device_count = {'GPU': gpus  , 'CPU': cores} , intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "  sess = tf.compat.v1.Session(config=config)\n",
    "  tf.compat.v1.keras.backend.set_session(sess)\n",
    "\n",
    "set_seeds_and_trace()\n",
    "set_session_with_gpus_and_cores()\n",
    "warnings.filterwarnings('ignore')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uvVC_dlqmdf6"
   },
   "source": [
    "## Attention *a la Bahdanau*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ibxTc61LmmhK"
   },
   "source": [
    "The easiest way to test a Layer in Keras is to create a simple model that uses such a layer, we will do just that! This also shows how easy is to add attention to your models, which we will use extensively when creating THE Transformer from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oLbdmNwEoUM7"
   },
   "source": [
    "Notice we need a custom model class because the inputs needs to be the query and value, and they could have different embeddings as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "36P0ESmxThvQ"
   },
   "outputs": [],
   "source": [
    "class AttentionModel(tf.keras.Model):\n",
    "\n",
    "  def __init__(self, vocab_size, max_tokens, embedding_dim, dropout_rate):\n",
    "    super().__init__()\n",
    "    self.embedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_tokens)\n",
    "    self.attention = tf.keras.layers.AdditiveAttention()\n",
    "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "    self.dense = tf.keras.layers.Dense(vocab_size, activation='softmax')\n",
    "\n",
    "  def call(self, inputs, training=False):\n",
    "\n",
    "    query, value = inputs\n",
    "    # Query embeddings of shape [batch_size, Tq, dimension].\n",
    "    query_embeddings = self.embedding(query)\n",
    "    # Value embeddings of shape [batch_size, Tv, dimension].\n",
    "    value_embeddings = self.embedding(value)\n",
    "    # Notice we could have an embedding for the inputs and another embedding for outputs, we will see more of that later\n",
    "    x = self.attention([query_embeddings, value_embeddings])\n",
    "    x = self.dropout(x, training=training)\n",
    "    x = self.dense(x, training=training)\n",
    "    return x\n",
    "\n",
    "  def build_graph(self, max_tokens, embedding_dim):\n",
    "    query_input = tf.keras.Input(shape=(None, max_tokens, embedding_dim), dtype='int32')\n",
    "    value_input = tf.keras.Input(shape=(None, max_tokens, embedding_dim), dtype='int32')\n",
    "    x = (query_input, value_input)\n",
    "    return Model(inputs=x, outputs=self.call(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WpLSon3cNVvc"
   },
   "outputs": [],
   "source": [
    "model = AttentionModel(100, 10, 20, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CSRhli0ZWFxf"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eXOb_IEGqV2p"
   },
   "source": [
    "Oh no! We need to call the model, well that is simple let's simulate 3 sentences!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mtn3d4wwyEGU"
   },
   "outputs": [],
   "source": [
    "embedding_dim = 20\n",
    "max_tokens = 10\n",
    "query = tf.constant(np.random.randint(0, embedding_dim, size=(3,max_tokens)))\n",
    "value = tf.constant(np.random.randint(0, embedding_dim, size=(3,max_tokens)))\n",
    "\n",
    "response = model((query,value) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8ngnlesuSTyC"
   },
   "outputs": [],
   "source": [
    "response.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bZCFervbyf7i"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gi07wCzxXNt7"
   },
   "outputs": [],
   "source": [
    "model.build_graph(max_tokens=max_tokens, embedding_dim=embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X1PW8NYaXOTq"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nWQHcNquqfVK"
   },
   "source": [
    "Notice that attention adds very few parameters, adds many knowledge to the following layers, and is paralellizable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A5y1okQ9QJ48"
   },
   "source": [
    "## MultiHead Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bakdsYuhqogu"
   },
   "source": [
    "Now you are ready to see Multi Head Attention. The idea is quite simple, as in CNNs we had many filters and each convolution checked many different aspects of an image, having many self attentions can check different aspects of our entity, globally. In image it is:\n",
    "\n",
    "<figure>\n",
    "<center>\n",
    "<img src='https://www.dropbox.com/s/wjfxpap06viclhv/mha.png?raw=1'  />\n",
    "<figcaption>Attention</figcaption></center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S4tqYQY3rPCJ"
   },
   "source": [
    "Each head performs Scaled attention as we did before with the weird formula, and then we concatenate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7TqMEi2gQNFZ"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttentionModel(tf.keras.Model):\n",
    "\n",
    "  def __init__(self, num_heads, vocab_size, attention_dim, max_tokens, embedding_dim, dropout_rate):\n",
    "    super().__init__()\n",
    "    self.embedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_tokens)\n",
    "\n",
    "    # key_dim stands for size of each attention head for query and key, we can also pass value_key is K!=V\n",
    "    self.attention = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=attention_dim, dropout=dropout_rate)\n",
    "\n",
    "    self.dense = tf.keras.layers.Dense(100, activation='softmax')\n",
    "\n",
    "  def call(self, inputs, training=False):\n",
    "\n",
    "    query, value = inputs\n",
    "    # Query embeddings of shape [batch_size, Tq, dimension].\n",
    "    query_embeddings = self.embedding(query)\n",
    "    # Value embeddings of shape [batch_size, Tv, dimension].\n",
    "    value_embeddings = self.embedding(value)\n",
    "    x, weights = self.attention(query_embeddings, value_embeddings, return_attention_scores=True)  # We return the scores to do our plot!\n",
    "    x = self.dense(x, training=training)\n",
    "    return x, weights\n",
    "\n",
    "  def build_graph(self, max_tokens, embedding_dim):\n",
    "    query_input = tf.keras.Input(shape=(max_tokens, embedding_dim), dtype='int32')\n",
    "    value_input = tf.keras.Input(shape=(max_tokens, embedding_dim), dtype='int32')\n",
    "    x = (query_input, value_input)\n",
    "    return Model(inputs=x, outputs=self.call(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HyWkokOtw6GW"
   },
   "outputs": [],
   "source": [
    "vocab_size=100\n",
    "model = MultiHeadAttentionModel(num_heads=3, vocab_size=vocab_size, attention_dim=2, max_tokens=max_tokens, embedding_dim=embedding_dim, dropout_rate=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vXhcalSwxF9a"
   },
   "outputs": [],
   "source": [
    "model.build_graph(max_tokens=max_tokens, embedding_dim=embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HW4raKPVxPNj"
   },
   "outputs": [],
   "source": [
    "query = tf.constant(np.random.randint(0,vocab_size, size=(3,max_tokens, 10)))\n",
    "value = tf.constant(np.random.randint(0,vocab_size, size=(3,max_tokens, 10)))\n",
    "\n",
    "response, weights = model((query,value) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HelBLsdnxbL7"
   },
   "outputs": [],
   "source": [
    "response.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NrYWnu1RsQWS"
   },
   "source": [
    "**Can you guess each value in the response.shape where does it come from?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OG5X7dchyN4v"
   },
   "outputs": [],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yVfmU78bsaIG"
   },
   "source": [
    "**And for the weights??**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "esLeIv1IyOtf"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n-KQ5uhrsfmB"
   },
   "source": [
    "Again, notice Attention as complex as multi head attention did not add many params and adds a lot lexical intelligence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uluP5iZYsn44"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPj5ESFtz13HLkVzxJhQqT0",
   "collapsed_sections": [
    "JsTW2QHimUlx"
   ],
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
