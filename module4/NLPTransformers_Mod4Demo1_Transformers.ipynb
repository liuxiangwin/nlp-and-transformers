{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/axel-sirota/nlp-and-transformers/blob/main/module4/NLPTransformers_Mod4Demo1_Transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gzf37X_XizQQ"
   },
   "source": [
    "# Transformers\n",
    "\n",
    "Â© Data Trainers LLC. GPL v 3.0.\n",
    "\n",
    "Author: Axel Sirota\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "Inspired highly on the tutorial [NMT with Transformers](https://www.tensorflow.org/text/tutorials/transformer) which takes the code from the original Transformer model paper originally proposed in [\"Attention is all you need\"](https://arxiv.org/abs/1706.03762) by Vaswani et al. (2017)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ovyaAhLpa55P"
   },
   "source": [
    "## Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "C9BTEOu0PerV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/app-root/lib/python3.9/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /opt/app-root/lib/python3.9/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/app-root/lib/python3.9/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/app-root/lib/python3.9/site-packages (from nltk) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in /opt/app-root/lib/python3.9/site-packages (from nltk) (4.66.1)\n",
      "Requirement already satisfied: gensim==4.2.0 in /opt/app-root/lib/python3.9/site-packages (4.2.0)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /opt/app-root/lib/python3.9/site-packages (from gensim==4.2.0) (1.24.4)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /opt/app-root/lib/python3.9/site-packages (from gensim==4.2.0) (1.11.4)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/app-root/lib/python3.9/site-packages (from gensim==4.2.0) (6.4.0)\n",
      "Requirement already satisfied: keras-nlp in /opt/app-root/lib/python3.9/site-packages (0.8.2)\n",
      "Requirement already satisfied: keras-core in /opt/app-root/lib/python3.9/site-packages (from keras-nlp) (0.1.7)\n",
      "Requirement already satisfied: absl-py in /opt/app-root/lib/python3.9/site-packages (from keras-nlp) (1.4.0)\n",
      "Requirement already satisfied: numpy in /opt/app-root/lib/python3.9/site-packages (from keras-nlp) (1.24.4)\n",
      "Requirement already satisfied: packaging in /opt/app-root/lib/python3.9/site-packages (from keras-nlp) (23.2)\n",
      "Requirement already satisfied: regex in /opt/app-root/lib/python3.9/site-packages (from keras-nlp) (2023.12.25)\n",
      "Requirement already satisfied: rich in /opt/app-root/lib/python3.9/site-packages (from keras-nlp) (12.6.0)\n",
      "Requirement already satisfied: dm-tree in /opt/app-root/lib/python3.9/site-packages (from keras-nlp) (0.1.8)\n",
      "Requirement already satisfied: kagglehub in /opt/app-root/lib/python3.9/site-packages (from keras-nlp) (0.2.0)\n",
      "Requirement already satisfied: tensorflow-text in /opt/app-root/lib/python3.9/site-packages (from keras-nlp) (2.16.1)\n",
      "Requirement already satisfied: requests in /opt/app-root/lib/python3.9/site-packages (from kagglehub->keras-nlp) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /opt/app-root/lib/python3.9/site-packages (from kagglehub->keras-nlp) (4.66.1)\n",
      "Requirement already satisfied: namex in /opt/app-root/lib/python3.9/site-packages (from keras-core->keras-nlp) (0.0.7)\n",
      "Requirement already satisfied: h5py in /opt/app-root/lib/python3.9/site-packages (from keras-core->keras-nlp) (3.10.0)\n",
      "Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in /opt/app-root/lib/python3.9/site-packages (from rich->keras-nlp) (0.9.1)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /opt/app-root/lib/python3.9/site-packages (from rich->keras-nlp) (2.17.2)\n",
      "Requirement already satisfied: tensorflow<2.17,>=2.16.1 in /opt/app-root/lib/python3.9/site-packages (from tensorflow-text->keras-nlp) (2.16.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/app-root/lib/python3.9/site-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text->keras-nlp) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /opt/app-root/lib/python3.9/site-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text->keras-nlp) (24.3.7)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/app-root/lib/python3.9/site-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text->keras-nlp) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/app-root/lib/python3.9/site-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text->keras-nlp) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/app-root/lib/python3.9/site-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text->keras-nlp) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in /opt/app-root/lib/python3.9/site-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text->keras-nlp) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/app-root/lib/python3.9/site-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text->keras-nlp) (3.3.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/app-root/lib/python3.9/site-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text->keras-nlp) (4.25.3)\n",
      "Requirement already satisfied: setuptools in /opt/app-root/lib/python3.9/site-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text->keras-nlp) (68.1.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/app-root/lib/python3.9/site-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text->keras-nlp) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/app-root/lib/python3.9/site-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text->keras-nlp) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/app-root/lib/python3.9/site-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text->keras-nlp) (4.9.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/app-root/lib/python3.9/site-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text->keras-nlp) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/app-root/lib/python3.9/site-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text->keras-nlp) (1.60.1)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in /opt/app-root/lib/python3.9/site-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text->keras-nlp) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in /opt/app-root/lib/python3.9/site-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text->keras-nlp) (3.0.5)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/app-root/lib/python3.9/site-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text->keras-nlp) (0.36.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/app-root/lib/python3.9/site-packages (from requests->kagglehub->keras-nlp) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/app-root/lib/python3.9/site-packages (from requests->kagglehub->keras-nlp) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/app-root/lib/python3.9/site-packages (from requests->kagglehub->keras-nlp) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/app-root/lib/python3.9/site-packages (from requests->kagglehub->keras-nlp) (2024.2.2)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/app-root/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow<2.17,>=2.16.1->tensorflow-text->keras-nlp) (0.41.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/app-root/lib/python3.9/site-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16.1->tensorflow-text->keras-nlp) (3.5.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/app-root/lib/python3.9/site-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16.1->tensorflow-text->keras-nlp) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/app-root/lib/python3.9/site-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16.1->tensorflow-text->keras-nlp) (3.0.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/app-root/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16.1->tensorflow-text->keras-nlp) (7.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/app-root/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16.1->tensorflow-text->keras-nlp) (2.1.5)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/app-root/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16.1->tensorflow-text->keras-nlp) (3.17.0)\n",
      "Requirement already satisfied: Keras-Preprocessing in /opt/app-root/lib/python3.9/site-packages (1.1.2)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /opt/app-root/lib/python3.9/site-packages (from Keras-Preprocessing) (1.24.4)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/app-root/lib/python3.9/site-packages (from Keras-Preprocessing) (1.16.0)\n",
      "Requirement already satisfied: tensorflow-text in /opt/app-root/lib/python3.9/site-packages (2.16.1)\n",
      "Requirement already satisfied: tensorflow<2.17,>=2.16.1 in /opt/app-root/lib/python3.9/site-packages (from tensorflow-text) (2.16.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/app-root/lib/python3.9/site-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/app-root/lib/python3.9/site-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /opt/app-root/lib/python3.9/site-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (24.3.7)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/app-root/lib/python3.9/site-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/app-root/lib/python3.9/site-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /opt/app-root/lib/python3.9/site-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (3.10.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/app-root/lib/python3.9/site-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in /opt/app-root/lib/python3.9/site-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/app-root/lib/python3.9/site-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (3.3.0)\n",
      "Requirement already satisfied: packaging in /opt/app-root/lib/python3.9/site-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/app-root/lib/python3.9/site-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/app-root/lib/python3.9/site-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (2.31.0)\n",
      "Requirement already satisfied: setuptools in /opt/app-root/lib/python3.9/site-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (68.1.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/app-root/lib/python3.9/site-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/app-root/lib/python3.9/site-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/app-root/lib/python3.9/site-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (4.9.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/app-root/lib/python3.9/site-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/app-root/lib/python3.9/site-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (1.60.1)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in /opt/app-root/lib/python3.9/site-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in /opt/app-root/lib/python3.9/site-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (3.0.5)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/app-root/lib/python3.9/site-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (0.36.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /opt/app-root/lib/python3.9/site-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (1.24.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/app-root/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow<2.17,>=2.16.1->tensorflow-text) (0.41.3)\n",
      "Requirement already satisfied: rich in /opt/app-root/lib/python3.9/site-packages (from keras>=3.0.0->tensorflow<2.17,>=2.16.1->tensorflow-text) (12.6.0)\n",
      "Requirement already satisfied: namex in /opt/app-root/lib/python3.9/site-packages (from keras>=3.0.0->tensorflow<2.17,>=2.16.1->tensorflow-text) (0.0.7)\n",
      "Requirement already satisfied: dm-tree in /opt/app-root/lib/python3.9/site-packages (from keras>=3.0.0->tensorflow<2.17,>=2.16.1->tensorflow-text) (0.1.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/app-root/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16.1->tensorflow-text) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/app-root/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16.1->tensorflow-text) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/app-root/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16.1->tensorflow-text) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/app-root/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16.1->tensorflow-text) (2024.2.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/app-root/lib/python3.9/site-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16.1->tensorflow-text) (3.5.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/app-root/lib/python3.9/site-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16.1->tensorflow-text) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/app-root/lib/python3.9/site-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16.1->tensorflow-text) (3.0.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/app-root/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16.1->tensorflow-text) (7.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/app-root/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16.1->tensorflow-text) (2.1.5)\n",
      "Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in /opt/app-root/lib/python3.9/site-packages (from rich->keras>=3.0.0->tensorflow<2.17,>=2.16.1->tensorflow-text) (0.9.1)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /opt/app-root/lib/python3.9/site-packages (from rich->keras>=3.0.0->tensorflow<2.17,>=2.16.1->tensorflow-text) (2.17.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/app-root/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16.1->tensorflow-text) (3.17.0)\n"
     ]
    }
   ],
   "source": [
    "# !pip install -U nltk 'gensim==4.2.0' 'keras-nlp' 'keras-preprocessing' 'tensorflow-text>=2.11'\n",
    "!pip install nltk\n",
    "!pip install gensim==4.2.0\n",
    "!pip install keras-nlp\n",
    "!pip install Keras-Preprocessing\n",
    "!pip install tensorflow-text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "0fpgYwAtNO2T"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /opt/app-\n",
      "[nltk_data]     root/src/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import multiprocessing\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Lambda, ELU, Conv1D, MaxPooling1D, Dropout\n",
    "from keras.preprocessing import sequence\n",
    "# from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from  tensorflow.keras.preprocessing.text import Tokenizer \n",
    "\n",
    "# from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "# tf.keras.layers.experimental\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import preprocessing\n",
    "from textblob import TextBlob, Word\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.initializers import Constant\n",
    "\n",
    "# from tensorflow.keras.layers.experimental import preprocessing\n",
    "# from tensorflow.keras.layers.experimental import preprocessing\n",
    "#ModuleNotFoundError: No module named 'tensorflow.keras.layers.experimental'\n",
    "\n",
    "from tensorflow.keras import Model, Input\n",
    "import tensorflow_text as tf_text\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import warnings\n",
    "import nltk\n",
    "import time\n",
    "\n",
    "TRACE = False\n",
    "\n",
    "def set_seeds_and_trace():\n",
    "  os.environ['PYTHONHASHSEED'] = '0'\n",
    "  np.random.seed(42)\n",
    "  tf.random.set_seed(42)\n",
    "  random.seed(42)\n",
    "  if TRACE:\n",
    "    tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "def set_session_with_gpus_and_cores():\n",
    "  cores = multiprocessing.cpu_count()\n",
    "  gpus = len(tf.config.list_physical_devices('GPU'))\n",
    "  config = tf.compat.v1.ConfigProto(device_count = {'GPU': gpus  , 'CPU': cores} , \n",
    "                                    intra_op_parallelism_threads=1,\n",
    "                                    inter_op_parallelism_threads=1)\n",
    "  sess = tf.compat.v1.Session(config=config)\n",
    "  tf.compat.v1.keras.backend.set_session(sess)\n",
    "\n",
    "# set_seeds_and_trace()\n",
    "# set_session_with_gpus_and_cores()\n",
    "warnings.filterwarnings('ignore')\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ekM1n0-JdYPN"
   },
   "source": [
    "## The Transformer Layers\n",
    "\n",
    "In this demo we will create, from scratch, with the same tools the original Authors had, the Transformer architecture. Why? To understand how it works, why it works, and exactly what is novel!\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "  <th colspan=1>The original Transformer diagram</th>\n",
    "  <th colspan=1>A representation of a 4-layer Transformer</th>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td>\n",
    "   <img width=400 src=\"https://www.tensorflow.org/images/tutorials/transformer/transformer.png\"/>\n",
    "  </td>\n",
    "  <td>\n",
    "   <img width=307 src=\"https://www.tensorflow.org/images/tutorials/transformer/Transformer-4layer-compact.png\"/>\n",
    "  </td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "Each of the components in these two diagrams will be explained as you progress through the demo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OFbNft2shTPp"
   },
   "source": [
    "### What did we have before?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q2FoDkeGhWJk"
   },
   "source": [
    "Before, we used Cross Attention or self attention, remember? And for sequence data we basically used it like this:\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "  <th colspan=1>Seq2Seq with attention</th>\n",
    "<tr>\n",
    "<tr>\n",
    "  <td>\n",
    "   <img src=\"https://www.dropbox.com/s/r6u7ll5nlt96t9f/seq2seq.png?raw=1\"/>\n",
    "  </td>\n",
    "</tr>\n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b3huYRrUjCAI"
   },
   "source": [
    "Where we input attention with the hidden state to create another updated hidden state we could input into the next cell. And this worked well on medium sized sentences, but was hard to train and unstable. Now that we know this, the Transformer basicaly tried to get rid of the RNN by using **only** attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vwmp1jC0dxVJ"
   },
   "source": [
    "### The embedding and positional encoding layer\n",
    "\n",
    "The inputs to both the encoder and decoder use the same embedding and positional encoding logic.\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "  <th colspan=1>The embedding and positional encoding layer</th>\n",
    "<tr>\n",
    "<tr>\n",
    "  <td>\n",
    "   <img src=\"https://www.tensorflow.org/images/tutorials/transformer/PositionalEmbedding.png\"/>\n",
    "  </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "UkI09F6zdXnb"
   },
   "outputs": [],
   "source": [
    "## This comes straight from the paper\n",
    "\n",
    "def positional_encoding(length, depth):\n",
    "  depth = depth/2\n",
    "\n",
    "  positions = np.arange(length)[:, np.newaxis]     # (seq, 1)\n",
    "  depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)\n",
    "\n",
    "  angle_rates = 1 / (10000**depths)         # (1, depth)\n",
    "  angle_rads = positions * angle_rates      # (pos, depth)\n",
    "\n",
    "  pos_encoding = np.concatenate(\n",
    "      [np.sin(angle_rads), np.cos(angle_rads)],\n",
    "      axis=-1)\n",
    "\n",
    "  return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "gLC8RPIkd--M"
   },
   "outputs": [],
   "source": [
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "  def __init__(self, vocab_size, d_model):\n",
    "    super().__init__()\n",
    "    self.d_model = d_model\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True)\n",
    "    self.pos_encoding = positional_encoding(length=2048, depth=d_model)\n",
    "\n",
    "  def compute_mask(self, *args, **kwargs):\n",
    "    return self.embedding.compute_mask(*args, **kwargs)\n",
    "\n",
    "  def call(self, x):\n",
    "    length = tf.shape(x)[1]\n",
    "    x = self.embedding(x)\n",
    "    # This factor sets the relative scale of the embedding and positonal_encoding.\n",
    "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "    x = x + self.pos_encoding[tf.newaxis, :length, :]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "GAnc5AKSeOvn"
   },
   "outputs": [],
   "source": [
    "pos = PositionalEmbedding(5000, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "zlIZmHLMemkR"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([3, 26, 100])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = tf.constant(np.random.randint(1,5000, size=(3,26)))\n",
    "response = pos(input)\n",
    "response.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "XnIEkVWqg59O"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 26), dtype=bool, numpy=\n",
       "array([[ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True]])>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response._keras_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yvlTAaqqhTBo"
   },
   "source": [
    "### Add and normalize\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "  <th colspan=2>Add and normalize</th>\n",
    "<tr>\n",
    "<tr>\n",
    "  <td>\n",
    "   <img src=\"https://www.tensorflow.org/images/tutorials/transformer/Add+Norm.png\"/>\n",
    "  </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P-ZTlyygheuT"
   },
   "source": [
    "Note: Use `Add` layer instead of + to propagate masks\n",
    "\n",
    "We will create a BaseAttention layer that inherits the Add+Norm and then each subclass of attention will implement the correct one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "OMMyVtc6hLaM"
   },
   "outputs": [],
   "source": [
    "class BaseAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, **kwargs):\n",
    "    super().__init__()\n",
    "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
    "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "    self.add = tf.keras.layers.Add()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L5Toc-Uukh15"
   },
   "source": [
    "### Self Attention layer\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "  <th colspan=1>The global self attention layer</th>\n",
    "<tr>\n",
    "<tr>\n",
    "  <td>\n",
    "   <img src=\"https://www.tensorflow.org/images/tutorials/transformer/SelfAttention.png\"/>\n",
    "  </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "RP4L1Mn-iFCl"
   },
   "outputs": [],
   "source": [
    "class GlobalSelfAttention(BaseAttention):\n",
    "  def call(self, x):\n",
    "    # We need to compare everything with everything, therefore Q, K and V must be the input\n",
    "    attn_output = self.mha(\n",
    "        query=x,\n",
    "        value=x,\n",
    "        key=x)\n",
    "    x = self.add([x, attn_output])  # This one comes from the base class\n",
    "    x = self.layernorm(x)  # This one comes from the base class\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "316Iwonjm6JY"
   },
   "source": [
    "Let's test it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "goX_DO0Sk8UB"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([3, 26, 100])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim = 100\n",
    "vocab_size = 5000\n",
    "input = tf.constant(np.random.randint(1,vocab_size, size=(3,26)))\n",
    "\n",
    "# First we apply the PositionalEmbedding to embed into what the attention layer expects\n",
    "pos = PositionalEmbedding(vocab_size, embedding_dim)\n",
    "\n",
    "# Then we do the self attention, the n_heads is arbitrary\n",
    "gsa = GlobalSelfAttention(num_heads=3, key_dim=embedding_dim)\n",
    "\n",
    "\n",
    "response = gsa(pos(input))\n",
    "response.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n5lGk8ZMle7K"
   },
   "source": [
    "Notice the shape is the same, since MHA concats all 3 heads and the we add everything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J76TA2ZMlrke"
   },
   "source": [
    "### The cross attention layer\n",
    "\n",
    "This layer connects the encoder and decoder. This layer is the most straight-forward use of attention in the model, it performs the same task as the attention block in the previous demo (and we will copy it).\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "  <th colspan=1>The cross attention layer</th>\n",
    "<tr>\n",
    "<tr>\n",
    "  <td>\n",
    "   <img src=\"https://www.tensorflow.org/images/tutorials/transformer/CrossAttention.png\"/>\n",
    "  </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "OxZMLVBRlMPz"
   },
   "outputs": [],
   "source": [
    "class CrossAttention(BaseAttention):\n",
    "  def call(self, x, context):\n",
    "    attn_output, attn_scores = self.mha(\n",
    "        query=x,\n",
    "        key=context,  # This is the key part!!\n",
    "        value=context,  # This is the key part!!\n",
    "        return_attention_scores=True)\n",
    "\n",
    "    # Cache the attention scores for plotting later.\n",
    "    self.last_attn_scores = attn_scores\n",
    "\n",
    "    x = self.add([x, attn_output])\n",
    "    x = self.layernorm(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "R9v9MPbvmVJw"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([3, 24, 512])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim_es = 100\n",
    "vocab_size_es = 5000\n",
    "\n",
    "embedding_dim_en = 512\n",
    "vocab_size_en = 6000\n",
    "\n",
    "# We are supposing the model will translate Spanish to English, so context for CrossAttention will be the spanish input.\n",
    "\n",
    "input_es = tf.constant(np.random.randint(1,vocab_size_es, size=(3,26)))\n",
    "input_en = tf.constant(np.random.randint(1,vocab_size_es, size=(3,24)))\n",
    "\n",
    "\n",
    "pos_es = PositionalEmbedding(vocab_size_es, embedding_dim_es)\n",
    "pos_en = PositionalEmbedding(vocab_size_en, embedding_dim_en)\n",
    "\n",
    "\n",
    "gsa = GlobalSelfAttention(num_heads=3, key_dim=embedding_dim_es)\n",
    "cross = CrossAttention(num_heads=3, key_dim=embedding_dim_en)\n",
    "\n",
    "\n",
    "context = gsa(pos_es(input_es)) # Forget about the feed forwards\n",
    "\n",
    "response = cross(pos_en(input_en), context=context) # Forget about masked attention for now, assume it is the identity\n",
    "\n",
    "response.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e5bXoAwvoD8T"
   },
   "source": [
    "Notice the shape is (batch_size, words in sentence in output, embedding_dim) , regardless the input sentence had more words or other embedding dim. We are doing a good move forward!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PuNqXT82oaRo"
   },
   "source": [
    "### The causal self attention layer (Masked Multi Headed Attention)\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "  <th colspan=1>The causal self attention layer</th>\n",
    "<tr>\n",
    "<tr>\n",
    "  <td>\n",
    "   <img src=\"https://www.tensorflow.org/images/tutorials/transformer/CausalSelfAttention.png\"/>\n",
    "  </td>\n",
    "</tr>\n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_-jDIHkJowAP"
   },
   "source": [
    "The only big difference in the masked multi headedd attention is that we cannot attend to words in the future, so we will use a mask such that the `Nth` word can only see the first `N-1` words and not all the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "z6N9U3qvn9Ui"
   },
   "outputs": [],
   "source": [
    "class CausalSelfAttention(BaseAttention):\n",
    "  def call(self, x):\n",
    "    attn_output = self.mha(\n",
    "        query=x,\n",
    "        value=x,\n",
    "        key=x,\n",
    "        use_causal_mask = True)  # This is the key!\n",
    "    x = self.add([x, attn_output])\n",
    "    x = self.layernorm(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PgCBG1b-osuv"
   },
   "source": [
    "<table>\n",
    "<tr>\n",
    "  <th colspan=1>The causal self attention layer</th>\n",
    "<tr>\n",
    "<tr>\n",
    "  <td>\n",
    "   <img width=330 src=\"https://www.tensorflow.org/images/tutorials/transformer/CausalSelfAttention-new-full.png\"/>\n",
    "  </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AdE_KY2TpEPC"
   },
   "source": [
    "Notice in the diagram above how the query can onlly attend the values for the past"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "RaHIa0k4oqD1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([3, 24, 100])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim_en = 512\n",
    "vocab_size_en = 6000\n",
    "\n",
    "# We are supposing the model will translate Spanish to English, so context for CrossAttention will be the spanish input.\n",
    "\n",
    "input_en = tf.constant(np.random.randint(1,vocab_size_es, size=(3,24)))\n",
    "\n",
    "\n",
    "pos_en = PositionalEmbedding(vocab_size_en, embedding_dim_en)\n",
    "\n",
    "csa = CausalSelfAttention(num_heads =3, key_dim=embedding_dim_en)\n",
    "\n",
    "response = csa(pos_es(input_en))\n",
    "\n",
    "response.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c8_jJCg8pE6b"
   },
   "source": [
    "### The feed forward network\n",
    "\n",
    "The transformer also includes this point-wise feed-forward network in both the encoder and decoder:\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "  <th colspan=1>The feed forward network</th>\n",
    "<tr>\n",
    "<tr>\n",
    "  <td>\n",
    "   <img src=\"https://www.tensorflow.org/images/tutorials/transformer/FeedForward.png\"/>\n",
    "  </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "Wu5xFoRjpBty"
   },
   "outputs": [],
   "source": [
    "class FeedForward(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, dff, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "    self.seq = tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),\n",
    "      tf.keras.layers.Dense(d_model),\n",
    "      tf.keras.layers.Dropout(dropout_rate)\n",
    "    ])\n",
    "    self.add = tf.keras.layers.Add()\n",
    "    self.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "  def call(self, x):\n",
    "    x = self.add([x, self.seq(x)])\n",
    "    x = self.layer_norm(x)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Pm7uGPMpqwu"
   },
   "source": [
    "### The encoder layer\n",
    "\n",
    "The encoder contains a stack of `N` encoder layers. Where each `EncoderLayer` contains a `GlobalSelfAttention` and `FeedForward` layer:\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "  <th colspan=1>The encoder layer</th>\n",
    "<tr>\n",
    "<tr>\n",
    "  <td>\n",
    "   <img src=\"https://www.tensorflow.org/images/tutorials/transformer/EncoderLayer.png\"/>\n",
    "  </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "3SJl4aJ_pSR0"
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self,*, d_model, num_heads, dff, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "\n",
    "    self.self_attention = GlobalSelfAttention(\n",
    "        num_heads=num_heads,\n",
    "        key_dim=d_model,\n",
    "        dropout=dropout_rate)\n",
    "\n",
    "    self.ffn = FeedForward(d_model, dff)\n",
    "\n",
    "  def call(self, x):\n",
    "    x = self.self_attention(x)\n",
    "    x = self.ffn(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "4NUNWld5uOBo"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([3, 26, 100])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim = 100\n",
    "vocab_size = 5000\n",
    "input = tf.constant(np.random.randint(1,vocab_size, size=(3,26)))\n",
    "pos = PositionalEmbedding(vocab_size, embedding_dim)\n",
    "sample_encoder_layer = EncoderLayer(d_model=embedding_dim, num_heads=3, dff=1012)\n",
    "response = sample_encoder_layer(pos(input))\n",
    "response.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-MzC1z-vuuxF"
   },
   "source": [
    "### The encoder\n",
    "\n",
    "Notice we need to be able to repeat the past EncoderLayer Nx times, so we need another Layer that is able to do exactly that\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "  <th colspan=1>The encoder</th>\n",
    "<tr>\n",
    "<tr>\n",
    "  <td>\n",
    "   <img src=\"https://www.tensorflow.org/images/tutorials/transformer/Encoder.png\"/>\n",
    "  </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "0TlZP_vVulm0"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, *, num_layers, d_model, num_heads,\n",
    "               dff, vocab_size, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.num_layers = num_layers\n",
    "\n",
    "    self.pos_embedding = PositionalEmbedding(\n",
    "        vocab_size=vocab_size, d_model=d_model)\n",
    "\n",
    "    self.enc_layers = [\n",
    "        EncoderLayer(d_model=d_model,\n",
    "                     num_heads=num_heads,\n",
    "                     dff=dff,\n",
    "                     dropout_rate=dropout_rate)\n",
    "        for _ in range(num_layers)]\n",
    "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "  def call(self, x):\n",
    "    # `x` is token-IDs shape: (batch, seq_len)\n",
    "    x = self.pos_embedding(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
    "\n",
    "    # Add dropout.\n",
    "    x = self.dropout(x)\n",
    "\n",
    "    for i in range(self.num_layers):\n",
    "      x = self.enc_layers[i](x)\n",
    "\n",
    "    return x  # Shape `(batch_size, seq_len, d_model)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "Yf9wGOSMwaEt"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([3, 26, 100])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim = 100\n",
    "vocab_size = 5000\n",
    "input = tf.constant(np.random.randint(1,vocab_size, size=(3,26)))\n",
    "sample_encoder = Encoder(num_layers=4,\n",
    "                         d_model=embedding_dim,\n",
    "                         num_heads=3,\n",
    "                         dff=512,\n",
    "                         vocab_size=vocab_size)\n",
    "response = sample_encoder(input)\n",
    "response.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uUU9CG_MwtJC"
   },
   "source": [
    "We got our Encoder!! Yahoo!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UR4MuroWw4p-"
   },
   "source": [
    "### The decoder layer\n",
    "\n",
    "Same as before we need a Decoder layer that uses the Attention layers and then another layer to permit having Nx layers of decoding\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "  <th colspan=1>The decoder layer</th>\n",
    "<tr>\n",
    "<tr>\n",
    "  <td>\n",
    "   <img src=\"https://www.tensorflow.org/images/tutorials/transformer/DecoderLayer.png\"/>\n",
    "  </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "_v5voBr2wywC"
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self,\n",
    "               *,\n",
    "               d_model,\n",
    "               num_heads,\n",
    "               dff,\n",
    "               dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "\n",
    "    self.causal_self_attention = CausalSelfAttention(\n",
    "        num_heads=num_heads,\n",
    "        key_dim=d_model,\n",
    "        dropout=dropout_rate)\n",
    "\n",
    "    self.cross_attention = CrossAttention(\n",
    "        num_heads=num_heads,\n",
    "        key_dim=d_model,\n",
    "        dropout=dropout_rate)\n",
    "\n",
    "    self.ffn = FeedForward(d_model, dff)\n",
    "\n",
    "  def call(self, x, context):\n",
    "    x = self.causal_self_attention(x=x)\n",
    "    x = self.cross_attention(x=x, context=context)\n",
    "\n",
    "    # Cache the last attention scores for plotting later\n",
    "    self.last_attn_scores = self.cross_attention.last_attn_scores\n",
    "\n",
    "    x = self.ffn(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "aP3yjQeuy2n4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([3, 24, 512])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim_es = 100\n",
    "vocab_size_es = 5000\n",
    "\n",
    "embedding_dim_en = 512\n",
    "vocab_size_en = 6000\n",
    "\n",
    "# We are supposing the model will translate Spanish to English, so context for CrossAttention will be the spanish input.\n",
    "\n",
    "input_es = tf.constant(np.random.randint(1,vocab_size_es, size=(3,26)))\n",
    "input_en = tf.constant(np.random.randint(1,vocab_size_es, size=(3,24)))\n",
    "\n",
    "pos_en = PositionalEmbedding(vocab_size_en, embedding_dim_en)\n",
    "\n",
    "\n",
    "encoder =  Encoder(num_layers=2, d_model=embedding_dim_es, num_heads=3, dff=512, vocab_size=vocab_size_es)\n",
    "\n",
    "context = encoder(input_es)\n",
    "\n",
    "decoder_layer = DecoderLayer(d_model=embedding_dim_en, num_heads=3, dff=218, dropout_rate=0.2)\n",
    "\n",
    "response = decoder_layer(pos_en(input_en), context=context)\n",
    "\n",
    "response.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pog5S-gQz9nF"
   },
   "source": [
    "### The Decoder\n",
    "\n",
    "Similar to the `Encoder`, the `Decoder` consists of a `PositionalEmbedding`, and a stack of `DecoderLayer`s:\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "  <th colspan=1>The embedding and positional encoding layer</th>\n",
    "<tr>\n",
    "<tr>\n",
    "  <td>\n",
    "   <img src=\"https://www.tensorflow.org/images/tutorials/transformer/Decoder.png\"/>\n",
    "  </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "dEZ2-NRbz1jz"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, *, num_layers, d_model, num_heads, dff, vocab_size,\n",
    "               dropout_rate=0.1):\n",
    "    super(Decoder, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.num_layers = num_layers\n",
    "\n",
    "    self.pos_embedding = PositionalEmbedding(vocab_size=vocab_size,\n",
    "                                             d_model=d_model)\n",
    "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "    self.dec_layers = [\n",
    "        DecoderLayer(d_model=d_model, num_heads=num_heads,\n",
    "                     dff=dff, dropout_rate=dropout_rate)\n",
    "        for _ in range(num_layers)]\n",
    "\n",
    "    self.last_attn_scores = None\n",
    "\n",
    "  def call(self, x, context):\n",
    "    # `x` is token-IDs shape (batch, target_seq_len)\n",
    "    x = self.pos_embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "    x = self.dropout(x)\n",
    "\n",
    "    for i in range(self.num_layers):\n",
    "      x  = self.dec_layers[i](x, context)\n",
    "\n",
    "    self.last_attn_scores = self.dec_layers[-1].last_attn_scores\n",
    "\n",
    "    # The shape of x is (batch_size, target_seq_len, d_model).\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "Mp4zkTw22-23"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([3, 24, 512])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim_es = 100\n",
    "vocab_size_es = 5000\n",
    "\n",
    "embedding_dim_en = 512\n",
    "vocab_size_en = 6000\n",
    "\n",
    "# We are supposing the model will translate Spanish to English, so context for CrossAttention will be the spanish input.\n",
    "\n",
    "input_es = tf.constant(np.random.randint(1,vocab_size_es, size=(3,26)))\n",
    "input_en = tf.constant(np.random.randint(1,vocab_size_es, size=(3,24)))\n",
    "\n",
    "encoder =  Encoder(num_layers=2, d_model=embedding_dim_es, num_heads=3, dff=512, vocab_size=vocab_size_es)\n",
    "\n",
    "context = encoder(input_es)\n",
    "\n",
    "decoder = Decoder(num_layers=3, d_model=embedding_dim_en, num_heads=5, dff=124, vocab_size=vocab_size_en)\n",
    "\n",
    "response = decoder(input_en, context=context)\n",
    "\n",
    "response.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3r2JLfz04Hsc"
   },
   "source": [
    "## The Transformer Model\n",
    "\n",
    "You now have `Encoder` and `Decoder`. To complete the `Transformer` model, you need to put them together and add a final linear (`Dense`) layer which converts the resulting vector at each location into output token probabilities.\n",
    "\n",
    "The output of the decoder is the input to this final linear layer.\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "  <th colspan=1>The transformer</th>\n",
    "<tr>\n",
    "<tr>\n",
    "  <td>\n",
    "   <img src=\"https://www.tensorflow.org/images/tutorials/transformer/transformer.png\"/>\n",
    "  </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "VrXSLwrF36u_"
   },
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "  def __init__(self, *, num_layers, d_model, num_heads, dff,\n",
    "               input_vocab_size, target_vocab_size, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "    self.encoder = Encoder(num_layers=num_layers, d_model=d_model,\n",
    "                           num_heads=num_heads, dff=dff,\n",
    "                           vocab_size=input_vocab_size,\n",
    "                           dropout_rate=dropout_rate)\n",
    "\n",
    "    self.decoder = Decoder(num_layers=num_layers, d_model=d_model,\n",
    "                           num_heads=num_heads, dff=dff,\n",
    "                           vocab_size=target_vocab_size,\n",
    "                           dropout_rate=dropout_rate)\n",
    "\n",
    "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "\n",
    "  def call(self, inputs):\n",
    "    # To use a Keras model with `.fit` you must pass all your inputs in the\n",
    "    # first argument.\n",
    "    context, x  = inputs\n",
    "\n",
    "    context = self.encoder(context)  # (batch_size, context_len, d_model)\n",
    "\n",
    "    x = self.decoder(x, context)  # (batch_size, target_len, d_model)\n",
    "\n",
    "    # Final linear layer output.\n",
    "    logits = self.final_layer(x)  # (batch_size, target_len, target_vocab_size)\n",
    "\n",
    "    # Return the final output and the attention weights.\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "7CIxbfVx9IBc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([3, 24, 6000])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim = 100\n",
    "vocab_size_es = 5000\n",
    "vocab_size_en = 6000\n",
    "\n",
    "num_layers = 4\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "dropout_rate = 0.1\n",
    "\n",
    "# We are supposing the model will translate Spanish to English, so context for CrossAttention will be the spanish input.\n",
    "\n",
    "input_es = tf.constant(np.random.randint(1,vocab_size_es, size=(3,26)))\n",
    "input_en = tf.constant(np.random.randint(1,vocab_size_es, size=(3,24)))\n",
    "\n",
    "transformer = Transformer(\n",
    "    num_layers=num_layers,\n",
    "    d_model=embedding_dim,\n",
    "    num_heads=num_heads,\n",
    "    dff=dff,\n",
    "    input_vocab_size=vocab_size_es,\n",
    "    target_vocab_size=vocab_size_en,\n",
    "    dropout_rate=dropout_rate)\n",
    "\n",
    "response = transformer((input_es, input_en))\n",
    "response.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "Gy5B7d_E-NDE"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"transformer_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"transformer_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">âââââââââââââââââââââââââââââââââââ³âââââââââââââââââââââââââ³ââââââââââââââââ\n",
       "â<span style=\"font-weight: bold\"> Layer (type)                    </span>â<span style=\"font-weight: bold\"> Output Shape           </span>â<span style=\"font-weight: bold\">       Param # </span>â\n",
       "â¡âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ©\n",
       "â encoder_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Encoder</span>)             â ?                      â     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,203,648</span> â\n",
       "âââââââââââââââââââââââââââââââââââ¼âââââââââââââââââââââââââ¼ââââââââââââââââ¤\n",
       "â decoder_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Decoder</span>)             â ?                      â     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,594,448</span> â\n",
       "âââââââââââââââââââââââââââââââââââ¼âââââââââââââââââââââââââ¼ââââââââââââââââ¤\n",
       "â dense_85 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                â ?                      â       <span style=\"color: #00af00; text-decoration-color: #00af00\">606,000</span> â\n",
       "âââââââââââââââââââââââââââââââââââ´âââââââââââââââââââââââââ´ââââââââââââââââ\n",
       "</pre>\n"
      ],
      "text/plain": [
       "âââââââââââââââââââââââââââââââââââ³âââââââââââââââââââââââââ³ââââââââââââââââ\n",
       "â\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ\n",
       "â¡âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ©\n",
       "â encoder_7 (\u001b[38;5;33mEncoder\u001b[0m)             â ?                      â     \u001b[38;5;34m2,203,648\u001b[0m â\n",
       "âââââââââââââââââââââââââââââââââââ¼âââââââââââââââââââââââââ¼ââââââââââââââââ¤\n",
       "â decoder_3 (\u001b[38;5;33mDecoder\u001b[0m)             â ?                      â     \u001b[38;5;34m3,594,448\u001b[0m â\n",
       "âââââââââââââââââââââââââââââââââââ¼âââââââââââââââââââââââââ¼ââââââââââââââââ¤\n",
       "â dense_85 (\u001b[38;5;33mDense\u001b[0m)                â ?                      â       \u001b[38;5;34m606,000\u001b[0m â\n",
       "âââââââââââââââââââââââââââââââââââ´âââââââââââââââââââââââââ´ââââââââââââââââ\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,404,096</span> (24.43 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m6,404,096\u001b[0m (24.43 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,404,096</span> (24.43 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m6,404,096\u001b[0m (24.43 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "transformer.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gkzGKLjN-T9k"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPJjI6PJYJ3HU2ZPWkTHiJV",
   "collapsed_sections": [
    "ekM1n0-JdYPN",
    "OFbNft2shTPp",
    "vwmp1jC0dxVJ",
    "yvlTAaqqhTBo",
    "L5Toc-Uukh15",
    "J76TA2ZMlrke",
    "PuNqXT82oaRo",
    "c8_jJCg8pE6b",
    "1Pm7uGPMpqwu",
    "-MzC1z-vuuxF",
    "UR4MuroWw4p-",
    "pog5S-gQz9nF"
   ],
   "gpuType": "A100",
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
